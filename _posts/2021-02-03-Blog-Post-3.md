---
layout: post
title: Blog Post 3 Web Scraping
---
In this post, we will perform some web scraping. The goal is to find the movies that share many same actors as one of my favorite movie: La La Land. In total, I wrote 3 parsing methods in the class ImdbSpider: parse, parse_full_credits, and parse_actor_pages. Let's take a look one by one.

This is the link to my GitHub repository: **https://github.com/jqlaura/Blog_Post_3**

Before we write the three methods, we will first create a new project using the code below: 

```python
conda activate PIC16B
scrapy startproject IMDB_scraper
cd IMDB_scraper
```
And then create a new *imdb_spiders.py* file to write our methods.
We will start with writing the following code:

```python
import scrapy

class ImdbSpider(scrapy.Spider):
    name = 'imdb_spider'
    start_urls = ['https://www.imdb.com/title/tt3783958/']
```
Where the url *https://www.imdb.com/title/tt3783958/* is the link to the La La Land IMDB page.

### Method 1. Parse

Our main parsing method takes us to the link of all the casts in the movie. This method does not return any data. It will simply navigate you from the movie’s main page to Cast&Crew Page, then call parse_full_credits(self, response) method to do the following operations. The Cast&Crew page would be the start_urls followed by fullcredits/?ref_=tt_ql_cl, and we will use response.urljoin() to merge them together. Here is how I implemented this method:


```python
def parse(self, response):
    '''
    This method assume that we start on a movie page, 
    and navigate to the Cast & Crew page.
    No data is returned; it calls another parsing method.
    '''
    #find using dev tool where the url is located, and get the link
    url = response.css("div.SubNav__SubNavContainer-sc-11106ua-1.hDUKxp").\
            css("li.ipc-inline-list__item")[0].css("a").attrib["href"]
    #join the url with the main site url
    url = response.urljoin(url)
    #yield the request
    yield scrapy.Request(url, callback = self.parse_full_credits)
```

With this parsing method 1, we are able to navigate to the Cast page where we can find a list of all the actors.

### Method 2. Parse_full_credits

To make this method work, we will need to write the second method below called *parse_full_credits(self, response)*. Similarly, this method also does not return any data. It will navigate from the Cast&Crew page to each actor’s personal page on IMDB, and call parse_actor_page(self, response) to do the next step. This second method is relatively simple, because it mainly gets all the urls for nativating to the pages of each specific actor. We will use the urls it generates in our method 3.


```python
def parse_full_credits(self,response):
    """
    This method assume that we start on the Cast & Crew page. 
    It will yield a scrapy.Request for navigation to the page of each actor listed on the Cast & Crew page. 
    No data is returned; it calls another parsing method.
    """
    #list of actors url
    actor_urls = [a.attrib["href"] for a in response.css("td.primary_photo a")]
    #call in each url the actor page method
    for url in actor_urls:
        url = response.urljoin(url)
        yield scrapy.Request(url, callback = self.parse_actor_page)
```

### Method 3. Parse_actor_page
This is the most important method, which will actually get data and return dictionaries containing the information of each actor and the name of their movie or tv shows. The difficulty here is that there are other sections connected right below the section "Actors", so we have to make use of the unique ids in order to separate each section.
Detailed explanation on how this method works will be given in the docstring and comments in the code below:

```python
def parse_actor_page(self, response):
        '''
        This method assumes that we start on the page of an actor.
        It should yield a dictionary with two key-value pairs for each movie or TV show that the actor played in.
        '''
        #get the name of the actor at the top of the page
        actor_name = response.xpath('//h1[@class="header"]/span/text()')\
            .extract_first()
        #get a list of the movie names by specifying that the id
        #starts with the word "actor"
        movie_name = response.css('div.filmo-row[id^="actor"] b a::text')\
            .extract()
        
        #loop over all the movie names and create a dictionary for each,
        #while the name of the actor is the same
        for name in movie_name:
            yield {
            "actor": actor_name,
            "movie_or_TV_name": name
                }
```

It took me a long long time to figure out the simple "id^" part. Anyways, I made it! And after finishing writing all the three methods, we run **scrapy crawl imdb_spider -o movies.csv** in command line to produce the csv file containing the (actor,movie) pair. To visualize the result a bit, we import the file into Jupyter notebook, and sort the dataframe with the top movies and TV shows that share actors with your favorite movie or TV show.


```python
import pandas as pd
```


```python
#import the csv file as a dataframe
```


```python
movies = pd.read_csv("movies.csv")
movies.head()
```


```python
#groupby movie name and count the number of actors
```


```python
movies = movies.groupby('movie_or_TV_name').count()
```


```python
#sort by descending order, reset index, rename
```


```python
movies = movies.sort_values(by = ['actor'],ascending = False).reset_index()
```


```python
movies = movies.rename(columns = {"actor":"number of shared actors"})
```


```python
movies[:10]
```

![Post_3.png](/images/Post_3.png)


The result for my movie of choice isn't as impressive as that of professor's Star Trek, but it also makes some reasonable suggestions on what movies I will probably like. This is the end of our Blog Post 3!
