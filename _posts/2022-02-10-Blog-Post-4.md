---
layout: post
title: Blog Post 4 Spectral Clustering
---

*Clustering* refers to the task of separating this data set into the two natural "blobs." K-means is a very common way to achieve this task, but when the clusters have weird shapes, K-means may not work well. In this blog post, we will create simple version of the *spectral clustering* algorithm for clustering data points. 


```python
#import necessary packages
import numpy as np
import sklearn
import scipy
from sklearn import datasets
from matplotlib import pyplot as plt
from sklearn.cluster import KMeans
```


```python
#create the bull-eye shaped clusters
n = 1000
X,y = datasets.make_circles(n_samples = n, shuffle = True,
                           noise = 0.05, random_state = None,
                           factor = 0.5)
plt.scatter(X[:,0],X[:,1])
```




    <matplotlib.collections.PathCollection at 0x7fcab5da0730>




    
![output_2_1.png](/images/output_2_1.png)
    


We can still make out two clusters in the data using K-means, but apparently they are NOT correct:


```python
km = KMeans(n_clusters = 2)
km.fit(X)
plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```




    <matplotlib.collections.PathCollection at 0x7fcab5eb27c0>




    
![output_4_1.png](/images/output_4_1.png)
    


As we'll see, spectral clustering is able to correctly cluster the two crescents. First of all, we will construct the **similarity matrix** that contains information about whether certain points are within epsilon distance of another point. In this problem we let epsilon = 0.4. Also, we don't want to use any for-loops in the function definition.


```python
def Similarity_matrix(X,epsilon):
    #calculate pairwise distances
    A = sklearn.metrics.pairwise_distances(X,X)
    #if distance > epsilon, record 0, else 1
    A = np.where(A > epsilon, 0, 1)
    np.fill_diagonal(A,0)
    return A
```


```python
A = Similarity_matrix(X,0.4)
```




    array([[0, 0, 0, ..., 0, 0, 0],
           [0, 0, 0, ..., 0, 0, 0],
           [0, 0, 0, ..., 0, 0, 0],
           ...,
           [0, 0, 0, ..., 0, 0, 0],
           [0, 0, 0, ..., 0, 0, 0],
           [0, 0, 0, ..., 0, 0, 0]])



The matrix A now contains information about which points are near (within distance epsilon) which other points. We now pose the task of clustering the data points in X as the task of partitioning the rows and columns of A. 

Now let me introduct the *binary norm cut objective* of a matrix $\mathbf{A}$, which is the function 

$$N_{\mathbf{A}}(C_0, C_1)\equiv \mathbf{cut}(C_0, C_1)\left(\frac{1}{\mathbf{vol}(C_0)} + \frac{1}{\mathbf{vol}(C_1)}\right)\;.$$

We define the *cut* function as follows:


```python
def cut(A,y):
    #make use of the indices in y
    B = A[y==0, :]
    B = B[:, y==1]
    return B.sum()
```

We also define functions to help us get the degree of the matrix A while returning a diagonal matrix. We will make use of the result later.


```python
def get_degree(A):
    degree = np.diag(sum(A))
    return degree
```

Let's check if D correctly records the degrees of A.


```python
D = get_degree(A)
D
```




    array([[ 64,   0,   0, ...,   0,   0,   0],
           [  0, 140,   0, ...,   0,   0,   0],
           [  0,   0, 109, ...,   0,   0,   0],
           ...,
           [  0,   0,   0, ...,  64,   0,   0],
           [  0,   0,   0, ...,   0,  59,   0],
           [  0,   0,   0, ...,   0,   0, 128]])



Let's move on to another element in the function, *vol*. By definition, The *volume* of cluster $C_0$ is a measure of how "big" cluster $C_0$ is. If we choose cluster $C_0$ to be small, then $\mathbf{vol}(C_0)$ will be small and $\frac{1}{\mathbf{vol}(C_0)}$ will be large, leading to an undesirable higher objective value. 


```python
def vols(A,y):
    #sum over axis = 0 (rows)
    d = np.sum(A,axis=0)
    #make use of indices in y
    v0 = d[y==0].sum()
    v1 = d[y==1].sum()
    return v0,v1
```

Now, we will define the normcut function using the previous equation we have. To see why this function is helpful, we create some artificial labels called rand, and compare the results using rand and the true label values in y.


```python
def normcut(A,y):
    #define the function using the given equation
    v0,v1 = vols(A,y)
    normcut = cut(A,y)*(1/v0+1/v1)
    return normcut
```


```python
n =1000
#create random labels
rand = np.random.randint(0,2,size = n)
cut1 = cut(A,y)
cut2 = cut(A,rand)
#compare the two cuts and vols
print(cut1,cut2)

v0, v1 = vols(A,y)
print(v0,v1)

#compare the two normcuts
normcut1 = normcut(A,y)
normcut2 = normcut(A,rand)
print(normcut1,normcut2)
```

    1165 24563
    32135 65607
    0.0540105578765737 1.0052927758240628


We shall see that with the true label values, the normcut is very small (~0.05), but with the randomly created cluster labels the normcut is much larger. This also applies to the cut value.

Now we define a new vector $\mathbf{z}$ such that: 

$$
z_i = 
\begin{cases}
    \frac{1}{\mathbf{vol}(C_0)} &\quad \text{if } y_i = 0 \\ 
    -\frac{1}{\mathbf{vol}(C_1)} &\quad \text{if } y_i = 1 \\ 
\end{cases}
$$


$$\mathbf{N}_{\mathbf{A}}(C_0, C_1) = \frac{\mathbf{z}^T (\mathbf{D} - \mathbf{A})\mathbf{z}}{\mathbf{z}^T\mathbf{D}\mathbf{z}}\;,$$

We want to write a function called to compute the appropriate $\mathbf{z}$ vector given A and y using the formula above.  


```python
def transform(A,y):
    #define the function according to the equation
    v0,v1 = vols(A,y)
    z = np.where(y==0,1/v0,-1/v1)
    return z
```


```python
#we check that the equation holds by equaling the two sides
z = transform(A,y)
normcut3 = (z @ (D-A) @ z)/(z @ D @ z)
print(np.isclose(normcut1, normcut3))

#also check if zTD1 == 0
normcut4 = z @ D @ np.ones(n)
print(np.isclose(normcut4,0))
```

    True
    True


As we have expected, the normcut1 value is close to normcut3, so the equation for normcut is valid.

The problem of minimizing the normcut objective is mathematically related to the problem of minimizing the function 

$$ R_\mathbf{A}(\mathbf{z})\equiv \frac{\mathbf{z}^T (\mathbf{D} - \mathbf{A})\mathbf{z}}{\mathbf{z}^T\mathbf{D}\mathbf{z}} $$

subject to the condition $\mathbf{z}^T\mathbf{D}\mathbb{1} = 0$. We will now minimize the function `orth_obj` with respect to $\mathbf{z}$. 


```python
e = np.ones(n)
d=D@e

def orth(u, v):
    return (u @ v) / (v @ v) * v

def orth_obj(z):
    #define the orth_obj function using the equation  
    z_o = z - orth(z, d)
    return (z_o @ (D - A) @ z_o)/(z_o @ D @ z_o)
```


```python
#Use the minimize function from scipy.optimize to minimize the function 
#took my macbook air 35 minutes to run...
z_min = scipy.optimize.minimize(orth_obj,rand)
```

Now let's check out the clustering result and see if we are close to being correct!


```python
label = np.where(z_min.x>0,1,0)
plt.scatter(X[:,0],X[:,1],c=label)
```




    <matplotlib.collections.PathCollection at 0x7fcab6864460>




    
![output_28_1.png](/images/output_28_1.png)
    


It seems like we only make a few mistakes which is acceptable.

Let's construct the (normalized) *Laplacian* matrix of the similarity matrix $\mathbf{A}$. Observe that the result gets even better. Only about 2 data points are incorrectly classified!


```python
L = np.linalg.inv(D) @ (D-A)
eigval, eigvec = np.linalg.eig(L)
#second smallest eigenvalue using index 1
z_eig = eigval[1].real
print(z_eig)
z_vec = np.real(eigvec[:,1]) 
plt.scatter(X[:,0], X[:,1], c = z_vec < 0)
```

    0.040919020830292134





    <matplotlib.collections.PathCollection at 0x7fcabc76bd60>




    
![output_31_2.png](/images/output_31_2.png)
    


FINALLY, we can write a function called spectral_clustering(X, epsilon) which takes in the input data X (in the same format as Part A) and the distance threshold epsilon and performs spectral clustering, returning an array of binary labels indicating whether data point i is in group 0 or group 1.


```python
def spectral_clustering(X, epsilon): 
    """
    input X: the input data
    input epsilon: threshold parameter
    output: an array of binary labels indicating whether
            data point is in group 0 or group 1.
    """
    #Construct the similarity matrix.
    A = Similarity_matrix(X,epsilon)
    #construct the degree m
    D = get_degree(A)
    #Construct the Laplacian matrix
    L = np.linalg.inv(D) @ (D-A)
    #Compute the eigenvector with second-smallest eigenvalue of the 
    #Laplacian matrix.
    eigval, eigvec = np.linalg.eig(L)
    eigvec = np.real(eigvec)
    c = eigvec[:,1]
    #return labels based on this eigenvector.
    label = np.where(c>0,1,0)
    return label
```


```python
#two extra functions that will help with future plotting
def plot(X,label):
    plt.scatter(X[:,0],X[:,1],c=label)
    correct = np.isclose(y,label)
    plt.title('The correct rate:{:.0%}'.format(correct.sum()/1000))

def correct_rate(X,y,label):
    correct = np.isclose(y,label)
    return('the correct rate is {:.0%}'.format(correct.sum()/1000))
```

For testing purpose,we run a few experiments using your function, by generating different data sets using **make_moons**.


```python
n = 1000
plt.figure(figsize=(12,8))

X1,y1 = datasets.make_moons(n_samples=n, shuffle=True, noise=0.05, random_state = None)
plt.subplot(221)
plt.title("noise=0.05")
plt.scatter(X1[:,0], X1[:,1])
X2,y2 = datasets.make_moons(n_samples=n, shuffle=True, noise=0.08, random_state = None)
plt.subplot(222)
plt.title("noise=0.08")
plt.scatter(X2[:,0], X2[:,1])
X3,y3 = datasets.make_moons(n_samples=n, shuffle=True, noise=0.1, random_state = None)
plt.subplot(223)
plt.title("noise=0.10")
plt.scatter(X3[:,0], X3[:,1])
X4,y4 = datasets.make_moons(n_samples=n, shuffle=True, noise=0.12, random_state = None)
plt.subplot(224)
plt.title("noise=0.12")
plt.scatter(X4[:,0],X4[:,1])
```




    <matplotlib.collections.PathCollection at 0x7fcaa13df1c0>




    
![output_36_1.png](/images/output_36_1.png)
    


When we increase the noise, notice that the cluster prediction still finds the correct shape, although the precision decreases.


```python
plt.figure(figsize = (12,8))

label1 = spectral_clustering(X1,0.4)
plt.subplot(221)
correct1 = correct_rate(X1,y1,label1)
plt.title("noise=0.05,"+correct1)
plt.scatter(X1[:,0],X1[:,1],c=label1)

label2 = spectral_clustering(X2,0.4)
plt.subplot(222)
correct2 = correct_rate(X2,y2,label2)
plt.title("noise=0.08,"+correct2)
plt.scatter(X2[:,0],X2[:,1],c=label2)

label3 = spectral_clustering(X3,0.4)
plt.subplot(223)
correct3 = correct_rate(X3,y3,label3)
plt.title("noise=0.1,"+correct3)
plt.scatter(X3[:,0],X3[:,1],c=label3)

label4 = spectral_clustering(X4,0.4)
plt.subplot(224)
correct4 = correct_rate(X4,y4,label4)
plt.title("noise=0.12,"+correct4)
plt.scatter(X4[:,0],X4[:,1],c=label4)
```




    <matplotlib.collections.PathCollection at 0x7fcaa3f40dc0>




    
![output_38_1.png](/images/output_38_1.png)
    


At last let's again visit the bull eye data:


```python
n = 1000
X, y = datasets.make_circles(n_samples=n, shuffle=True, noise=0.05, random_state=None, factor = 0.4)
plt.scatter(X[:,0], X[:,1])
```




    <matplotlib.collections.PathCollection at 0x7fca9f407ca0>




    
![output_40_1.png](/images/output_40_1.png)
    


There are two concentric circles. As before k-means will not do well here at all. 


```python
km = KMeans(n_clusters = 2)
km.fit(X)
plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```




    <matplotlib.collections.PathCollection at 0x7fca9f533910>




    
![output_42_1.png](/images/output_42_1.png)
    


We test spectral clustering's performance on the bull eye shaped clusters. We also change the parameter epsilon to see which value(s) create the best cluster predictions. We shall see that **epsilon = 0.4 and epsilon = 0.5** both work for our data.


```python
plt.figure(figsize = (12,8))

label1 = spectral_clustering(X,0.3)
correct1 = correct_rate(X,y,label1)
label2 = spectral_clustering(X,0.4)
correct2 = correct_rate(X,y,label2)
label3 = spectral_clustering(X,0.5)
correct3 = correct_rate(X,y,label3)
label4 = spectral_clustering(X,0.6)
correct4 = correct_rate(X,y,label4)
plt.subplot(221)
plt.title("epsilon=0.3,"+correct1)
plt.scatter(X[:,0],X[:,1],c=label1)
plt.subplot(222)
plt.title("epsilon=0.4,"+correct2)
plt.scatter(X[:,0],X[:,1],c=label2)
plt.subplot(223)
plt.title("epsilon=0.5,"+correct3)
plt.scatter(X[:,0],X[:,1],c=label3)
plt.subplot(224)
plt.title("epsilon=0.6,"+correct4)
plt.scatter(X[:,0],X[:,1],c=label4)
```




    <matplotlib.collections.PathCollection at 0x7fca9f47cf10>




    
![output_44_1.png](/images/output_44_1.png)
    


This is the end of our Blog Post 4!
